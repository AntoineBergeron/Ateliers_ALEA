density(matF)
density(matF[,1])
plot(density(matF[,1]))
marginal(matF[,1])
?marginal()
install.packages("discreteRV")
library(discreteRV) # Calcul de marginales
marginal(matF[,1])
AandB <- jointRV(outcomes = list(1:3, 0:2), probs = 1:9 / sum(1:9))
AandB
marginal(AandB, 1)
test1 <- dCopula(mat12,BB8Copula(param = c(1.65, 0.97)))
test2 <- dCopula(mat13, tCopula(0.61), df = 2.09)
test3 <- dCopula(mat23, normalCopula(-0.11))
matF[,1]*matF[,2]
matF[,1]*matF[,2]*matF[,3]*test1*test2*test3
?vinecop
fit$fitted
fit <- vinecop(matF, var_types = c("c","c","c"), par_method = "mle", family_set = "parametric")
summary(fit)
plot(fit)
contour(fit)
fit$fitted
summary(fit)[1]
summary(fit)[1,]
summary(fit)[,7]
summary(fit)[,8]
donnees <- read.table(file = 'C:/Users/bera1923/Desktop/CeCS - Consultations/Clients et Clientes/Zidane Toffa/donnees.txt', sep = "\t", header = TRUE)
# **** Changer file en utilisant le même jeu de données fournit par courriel ****
#Envoyer Attack en dernière colonne (pour utiliser la fonction de StepWise)
Att <- donnees[,1]                   # Copie la première colonne
donnees <- donnees[,-1]              # Enlève la première colonne
donneesFinal <- cbind(donnees,Att)   # Colle la copie à la fin du tableau
colnames(donneesFinal) <- c("RecBro", "Rxt", "Slot", "Snir", "Tim", "TotBusy", "TotLost", "Att")
# Noms des colonnes abrégés
# Créer un ensemble d'entraînement et de test pour comparer StepWise / Lasso
set.seed(12)
echaEntr <- donneesFinal$Att %>%
createDataPartition(p = 0.8, list = FALSE)    # 80% pour entraînement, 20% pour Test
donneesEntr  <- donneesFinal[echaEntr, ]  # données d'entraînement
donneesTest <- donneesFinal[-echaEntr, ]  # données test
# 1- Utilisons les données d'entraînement pour estimer les paramètres du modèle
# 2- Appliquons le modèle avec les paramètres estimés sur les données test
# Pour le modèle lasso
x <- model.matrix(Att~., donneesEntr)[,-1]   # Sur R, on doit séparer les variables x et y pour Lasso
y <- donneesEntr$Att                         # On obtient x et y ainsi
cv.lasso <- cv.glmnet(x, y, alpha = 1, family = "binomial")
# Fit le modèle final sur les données d'entraînement
model.lasso <- glmnet(x, y, alpha = 1, family = "binomial",
lambda = cv.lasso$lambda.min)
# Coefficients de la régression
coef(model.lasso)
# Prédictions
x.test <- model.matrix(Att ~., donneesTest)[,-1]
prob <- model.lasso %>% predict(newx = x.test)
ClassPred <- ifelse(prob > 0.5, 1, 0)
# Précision
ClassObs <- donneesTest$Att
mean(ClassPred == ClassObs)
# Stepwise model
step.model <- glm(Att ~., data = donneesEntr, family = binomial, control = glm.control(maxit = 50)) %>%
stepAIC(trace = TRUE)
# Coefficients de la régression
coef(step.model)
summary(step.model)
# Prédictions
prob <- predict(step.model, donneesTest, type = "response")
ClassPred <- ifelse(prob > 0.5, 1, 0)
# Précision
ClassObs <- donneesTest$Att
mean(ClassPred==donneesEntr$Att)
# Test de corrélation entre les variables explicatives (après test sur tous les cas possibles)
cor(donneesFinal, method = "pearson")
# Correlation forte entre Rxt et Slot
# Correlation forte entre Rxt et Tim
# Correlation forte entre Rxt et TotLost
# Correlation forte entre Slot et Tim
# Correlation forte entre Slot et TotLost
# Correlation forte entre Snir et TotLost
# Correlation forte entre Tim et TotLost
# Rxt - Slot - TotLost - Tim  semblent fortement corrélés entre elles. (linéairement)
# Test en utilisant seulement les variables RecBro, Rxt, Snir et TotBusy a été fait mais mène à la même conclusion.
### Le modèle Lasso propose d'utiliser les variables explicatives suivantes: RecBro, Rxt, et TotBusy ###
# Créeons notre nouveau tableau de données
col1 <- donneesFinal[,1]  # RecBro
col2 <- donneesFinal[,2]  # Rxt
col3 <- donneesFinal[,6]  # TotBusy
col4 <- donneesFinal[,8]  # Att
ntabFinal <- cbind.data.frame(col1,col2,col3,col4)           # coller les 4 colonnes
colnames(ntabFinal) <- c("RecBro", "Rxt", "TotBusy", "Att")  # renommer les colonnes
ntabFinal
### Partie de l'algorithme dans Article_Copule ###
# Étape 1: Estimation du p (par le pourcentage de succès):
es_p <- mean(ntabFinal[,4])   # Effectue la moyenne des Att (Somme des 0 et 1 divisé en n = 240 attaques)
es_p   # la probabilité d'attaque est de 0.2083333
# Étape 2: Estimation des marginales (par la distribution empirique redimensionnée):
Fn_1 <- ecdf(ntabFinal[,1])  # Fonction de répartition empirique
Fn_2 <- ecdf(ntabFinal[,2])  # Fonction de répartition empirique
Fn_3 <- ecdf(ntabFinal[,3])  # Fonction de répartition empirique
plot(Fn_1, verticals = TRUE, do.points = FALSE, main = "Fonction de répartition de la variable RecBro")
plot(Fn_2, verticals = TRUE, do.points = FALSE, main = "Fonction de répartition de la variable Rxt")
plot(Fn_3, verticals = TRUE, do.points = FALSE, main = "Fonction de répartition de la variable TotBusy")
matF <- matrix(NA, nrow = nrow(ntabFinal), ncol = ncol(ntabFinal)-1)     # Création de la matrice des répartition
matF[,1] <- nrow(ntabFinal) / (nrow(ntabFinal)+1) * Fn_1(ntabFinal[,1])  # première colonne (Rescaled distribution)
matF[,2] <- nrow(ntabFinal) / (nrow(ntabFinal)+1) * Fn_2(ntabFinal[,2])  # deuxième colonne (Rescaled distribution)
matF[,3] <- nrow(ntabFinal) / (nrow(ntabFinal)+1) * Fn_3(ntabFinal[,3])  # troisième colonne (Rescaled distribution)
matF
MatntabFinal <- as.matrix(ntabFinal)
# Étape 3: Estimation du paramètre de la copule:
# Copules: 1 = Gaussienne, 2 = Student, 3 = Gumbel, 4 = Clayton, 5 = Frank, 6 = Joe,
coefficient <- function(copule, matrice){
if(copule == 1){
cop <- normalCopula(dim = 3)
t <- "La copule gaussienne possède un paramètre égal à"
} else if(copule == 2){
cop <- tCopula(dim = 3)
t <- "La copule Student possède un paramètre égal à"
} else if(copule == 3){
cop <- gumbelCopula(dim = 3)
t <- "La copule Gumbel possède un paramètre égal à"
} else if(copule == 4){
cop <- claytonCopula(dim = 3)
t <- "La copule Clayton possède un paramètre égal à"
} else if(copule == 5){
cop <- frankCopula(dim = 3)
t <- "La copule de Frank possède un paramètre égal à"
} else if(copule == 6){
cop <- joeCopula(dim = 3)
t <- "La copule de Joe possède un paramètre égal à"
}
fit <- fitCopula(cop, matrice, method = "mpl")
return(list((paste(t, coef(fit), sep = " ")),coef(fit)))
}
coefficient(1,matF)
param_gauss <- unname(coefficient(1,matF)[2][[1]])  # paramètre de la copule gaussienne
coefficient(2,matF)                                 # possède le paramètre ainsi que le degré de liberté!
param_stud <- unname(coefficient(2,matF)[2][[1]])   # paramètres de la copule de Student
coefficient(3,matF)
param_gumb <- unname(coefficient(3,matF)[2][[1]])  # paramètre de la copule Gumbel
coefficient(4,matF)
param_clay <- unname(coefficient(4,matF)[2][[1]])  # paramètre de la copule Clayton
coefficient(5,matF)
param_frank <- unname(coefficient(5,matF)[2][[1]])  # paramètre de la copule Frank
coefficient(6,matF)
param_joe <- unname(coefficient(6,matF)[2][[1]])  # paramètre de la copule Joe
# Calcul de la vraisemblance que l'on cherche à minimiser par la suite à l'aide des critères AIC/BIC
t_max <- nrow(ntabFinal)
Func_gauss = function(x,t) log(dCopula(c(matF[t,1],matF[t,2],matF[t,3]),normalCopula(param_gauss, dim = 3)))
vrais_gauss = sum(sapply(1:t_max, function(t) Func_gauss(x,t)))
Func_stud = function(x,t) log(dCopula(c(matF[t,1],matF[t,2],matF[t,3]),tCopula(param = param_stud[1], df = param_stud[2] , dim = 3)))
vrais_stud = sum(sapply(1:t_max, function(t) Func_stud(x,t)))
Func_gumb = function(x,t) log(dCopula(c(matF[t,1],matF[t,2],matF[t,3]),gumbelCopula(param_gumb, dim = 3)))
vrais_gumb = sum(sapply(1:t_max, function(t) Func_gumb(x,t)))
Func_clay = function(x,t) log(dCopula(c(matF[t,1],matF[t,2],matF[t,3]),claytonCopula(param_clay, dim = 3)))
vrais_clay = sum(sapply(1:t_max, function(t) Func_clay(x,t)))
Func_frank = function(x,t) log(dCopula(c(matF[t,1],matF[t,2],matF[t,3]),frankCopula(param_frank, dim = 3)))
vrais_frank = sum(sapply(1:t_max, function(t) Func_frank(x,t)))
Func_joe = function(x,t) log(dCopula(c(matF[t,1],matF[t,2],matF[t,3]),joeCopula(param_joe, dim = 3)))
vrais_joe = sum(sapply(1:t_max, function(t) Func_joe(x,t)))
#Notons les résultats de vraisemblance dans un même vecteur
logvraisemblance_des_cop = c(vrais_gauss,vrais_stud,vrais_gumb,vrais_clay,vrais_frank,vrais_joe)
# Calculons l'AIC et le BIC
AIC_des_modeles = 2 - 2*(logvraisemblance_des_cop)
BIC_des_modeles = log(nrow(ntabFinal)) - 2 * logvraisemblance_des_cop
tableauAIC <- t(as.data.frame(AIC_des_modeles))
colnames(tableauAIC) <- c("Gauss", "Student", "Gumbel", "Clayton", "Frank", "Joe")
tableauAIC
tableauBIC <- t(as.data.frame(BIC_des_modeles))
colnames(tableauBIC) <- c("Gauss", "Student", "Gumbel", "Clayton", "Frank", "Joe")
tableauBIC
if(isTRUE(which.min(tableauAIC) == which.min(tableauBIC))){
copgagn <- which.min(tableauAIC)
} else
"Nous avons un problème au AIC/BIC"
dCopula(c(matF[1,1],matF[1,2],matF[1,3])
)
dCopula(c(matF[t,1],matF[t,2],matF[t,3]),normalCopula(param_gauss, dim = 3))
dCopula(c(matF[1,1],matF[1,2],matF[1,3]),normalCopula(param_gauss, dim = 3))
matF
matcop <- cbind(col4,matF)
col4[t]*log(cCopula(matcop,normalCopula(param_gauss, dim = 3)))
col4*log(cCopula(matcop,normalCopula(param_gauss, dim = 3)))
col4*log(cCopula(matcop,normalCopula(param_gauss, dim = 4)))
col4*log(cCopula(matcop,normalCopula(param_gauss, dim = 4))) + (1 - col4)*log(cCopula(matcop,normalCopula(param_gauss, dim = 4)))
col4[1]*log(cCopula(matcop[1,],normalCopula(param_gauss, dim = 4))) + (1 - col4[1])*log(cCopula(matcop[,1],normalCopula(param_gauss, dim = 4)))
col4[1]*log(dCopula(matcop[1,],normalCopula(param_gauss, dim = 4))) + (1 - col4[1])*log(dCopula(matcop[,1],normalCopula(param_gauss, dim = 4)))
cCopula(matcop, normalCopula(param = param_gauss))
1 - cCopula(matcop, normalCopula(param = param_gauss))
col5  <- c(1- es_p)
matcop <- cbind(col5,matF)
cCopula(matcop, normalCopula(param = param_gauss))
1 - cCopula(matcop, normalCopula(param = param_gauss))[,2]
col4 * (1 - cCopula(matcop, normalCopula(param = param_gauss))[,2]) + (1 - col4) * cCopula(matcop, normalCopula(param = param_gauss))[,2]
cCopula(matcop, normalCopula(param = param_gauss))
col4 * log((1 - cCopula(matcop, normalCopula(param = param_gauss))[,2])) + (1 - col4) * log(cCopula(matcop, normalCopula(param = param_gauss)))[,2]
(1 - cCopula(matcop, normalCopula(param = param_gauss))[,2])^col4 * ((cCopula(matcop, normalCopula(param = param_gauss)))[,2])^(1-col4)
sum((1 - cCopula(matcop, normalCopula(param = param_gauss))[,2])^col4 * ((cCopula(matcop, normalCopula(param = param_gauss)))[,2])^(1-col4))
(1 - cCopula(matcop, tCopula(param = param_stud[1], df = param_stud[2]))[,2])^col4 * ((cCopula(matcop, tCopula(param = param_stud[1], df = param_stud[2])))[,2])^(1-col4)
v_stud <- sum((1 - cCopula(matcop, tCopula(param = param_stud[1], df = param_stud[2]))[,2])^col4 * ((cCopula(matcop, tCopula(param = param_stud[1], df = param_stud[2])))[,2])^(1-col4)
)
v_gauss <- sum((1 - cCopula(matcop, normalCopula(param = param_gauss))[,2])^col4 * ((cCopula(matcop, normalCopula(param = param_gauss)))[,2])^(1-col4))
coefficient(1,donneesFinal[,-4])
(1 - cCopula(matcop, tCopula(param = param_stud[1], df = param_stud[2]))[,2])^col4 * ((cCopula(matcop, tCopula(param = param_stud[1], df = param_stud[2])))[,2])^(1-col4)
v_stud <- sum((1 - cCopula(matcop, tCopula(param = param_stud[1], df = param_stud[2]))[,2])^col4 * ((cCopula(matcop, tCopula(param = param_stud[1], df = param_stud[2])))[,2])^(1-col4)
)
(1 - cCopula(matcop[1,], normalCopula(param = param_gauss))[,2])^col4 * ((cCopula(matcop[1,], normalCopula(param = param_gauss)))[,2])^(1-col4)
cCopula(matcop[1,], normalCopula(param = param_gauss)))[,2])
((cCopula(matcop[1,], normalCopula(param = param_gauss))[,2])^(1-col4)
((cCopula(matcop[1,], normalCopula(param = param_gauss))[,2])^(1-col4))
1
((cCopula(matcop[1,], normalCopula(param = param_gauss))[,2])^(1-col4))
matcop
((cCopula(matcop[1,], normalCopula(param = param_gauss)))^(1-col4))
((cCopula(matcop[1,], normalCopula(param = param_gauss), indices = 1))^(1-col4))
((cCopula(matcop, normalCopula(param = param_gauss), indices = 1))^(1-col4))
((cCopula(matcop, normalCopula(param = param_gauss), indices = 2))^(1-col4))
((cCopula(matcop, normalCopula(param = param_gauss), indices = 3))^(1-col4))
((cCopula(matcop, normalCopula(param = param_gauss), indices = 2))^(1-col4))
((1 - cCopula(matcop, normalCopula(param = param_gauss), indices = 2))^(1-col4))
(1 - cCopula(matcop, normalCopula(param = param_gauss), indices = 2))^(1-col4)
(1 - cCopula(matcop, normalCopula(param = param_gauss), indices = 2))^(1-col4) * ((cCopula(matcop, normalCopula(param = param_gauss), indices = 2)))^(1-col4)
(1 - cCopula(matcop, normalCopula(param = param_gauss), indices = 2))^(col4) * ((cCopula(matcop, normalCopula(param = param_gauss), indices = 2)))^(1-col4)
cCopula(matcop, normalCopula(param = param_gauss))
cCopula(matcop, normalCopula(param = param_gauss), indices = 1)
cCopula(matcop, normalCopula(param = param_gauss), indices = 2)
cCopula(matcop, normalCopula(param = param_gauss))
fit <- vinecop(matF, var_types = c("c","c","c"), par_method = "mle", family_set = "parametric")
summary(fit)
plot(fit)
contour(fit)
tau <- 0.5
theta <- iTau(claytonCopula(), tau = tau)
d <- 2
cc <- claytonCopula(theta, dim = d)
n <- 1000
set.seed(271)
## A small u_1
u1 <- 0.05
U <- cCopula(cbind(u1, runif(n)), copula = cc, inverse = TRUE)
plot(U[,2], ylab = quote(U[2]))
u1 <- 0.95
U <- cCopula(cbind(u1, runif(n)), copula = cc, inverse = TRUE)
plot(U[,2], ylab = quote(U[2]))
tau <- 0.5
theta <- iTau(gumbelCopula(), tau = tau)
d <- 5
gc <- gumbelCopula(theta, dim = d)
n <- 200
set.seed(271)
U. <- matrix(runif(n*d), ncol = d) # U(0,1)^d
U.
# }
# NOT RUN {
## Transform to Gumbel sample via conditional distribution method
U <- cCopula(U., copula = gc, inverse = TRUE) # slow for ACs except Clayton
splom2(U) # scatter-plot matrix copula sample
## Rosenblatt transform back to U(0,1)^d (as a check)
U. <- cCopula(U, copula = gc)
splom2(U.)
donnees <- read.table(file = 'C:/Users/bera1923/Desktop/CeCS - Consultations/Clients et Clientes/Zidane Toffa/donnees.txt', sep = "\t", header = TRUE)
# **** Changer file en utilisant le même jeu de données fournit par courriel ****
#Envoyer Attack en dernière colonne (pour utiliser la fonction de StepWise)
Att <- donnees[,1]                   # Copie la première colonne
donnees <- donnees[,-1]              # Enlève la première colonne
donneesFinal <- cbind(donnees,Att)   # Colle la copie à la fin du tableau
colnames(donneesFinal) <- c("RecBro", "Rxt", "Slot", "Snir", "Tim", "TotBusy", "TotLost", "Att")
# Noms des colonnes abrégés
# Créer un ensemble d'entraînement et de test pour comparer StepWise / Lasso
set.seed(12)
echaEntr <- donneesFinal$Att %>%
createDataPartition(p = 0.8, list = FALSE)    # 80% pour entraînement, 20% pour Test
donneesEntr  <- donneesFinal[echaEntr, ]  # données d'entraînement
donneesTest <- donneesFinal[-echaEntr, ]  # données test
# 1- Utilisons les données d'entraînement pour estimer les paramètres du modèle
# 2- Appliquons le modèle avec les paramètres estimés sur les données test
# Pour le modèle lasso
x <- model.matrix(Att~., donneesEntr)[,-1]   # Sur R, on doit séparer les variables x et y pour Lasso
y <- donneesEntr$Att                         # On obtient x et y ainsi
cv.lasso <- cv.glmnet(x, y, alpha = 1, family = "binomial")
# Fit le modèle final sur les données d'entraînement
model.lasso <- glmnet(x, y, alpha = 1, family = "binomial",
lambda = cv.lasso$lambda.min)
# Coefficients de la régression
coef(model.lasso)
# Prédictions
x.test <- model.matrix(Att ~., donneesTest)[,-1]
prob <- model.lasso %>% predict(newx = x.test)
ClassPred <- ifelse(prob > 0.5, 1, 0)
# Précision
ClassObs <- donneesTest$Att
mean(ClassPred == ClassObs)
# Stepwise model
step.model <- glm(Att ~., data = donneesEntr, family = binomial, control = glm.control(maxit = 50)) %>%
stepAIC(trace = TRUE)
# Coefficients de la régression
coef(step.model)
summary(step.model)
# Prédictions
prob <- predict(step.model, donneesTest, type = "response")
ClassPred <- ifelse(prob > 0.5, 1, 0)
# Précision
ClassObs <- donneesTest$Att
mean(ClassPred==donneesEntr$Att)
# Test de corrélation entre les variables explicatives (après test sur tous les cas possibles)
cor(donneesFinal, method = "pearson")
# Correlation forte entre Rxt et Slot
# Correlation forte entre Rxt et Tim
# Correlation forte entre Rxt et TotLost
# Correlation forte entre Slot et Tim
# Correlation forte entre Slot et TotLost
# Correlation forte entre Snir et TotLost
# Correlation forte entre Tim et TotLost
# Rxt - Slot - TotLost - Tim  semblent fortement corrélés entre elles. (linéairement)
# Test en utilisant seulement les variables RecBro, Rxt, Snir et TotBusy a été fait mais mène à la même conclusion.
### Le modèle Lasso propose d'utiliser les variables explicatives suivantes: RecBro, Rxt, et TotBusy ###
# Créeons notre nouveau tableau de données
col1 <- donneesFinal[,1]  # RecBro
col2 <- donneesFinal[,2]  # Rxt
col3 <- donneesFinal[,6]  # TotBusy
col4 <- donneesFinal[,8]  # Att
ntabFinal <- cbind.data.frame(col1,col2,col3,col4)           # coller les 4 colonnes
colnames(ntabFinal) <- c("RecBro", "Rxt", "TotBusy", "Att")  # renommer les colonnes
ntabFinal
### Partie de l'algorithme dans Article_Copule ###
# Étape 1: Estimation du p (par le pourcentage de succès):
es_p <- mean(ntabFinal[,4])   # Effectue la moyenne des Att (Somme des 0 et 1 divisé en n = 240 attaques)
es_p   # la probabilité d'attaque est de 0.2083333
# Étape 2: Estimation des marginales (par la distribution empirique redimensionnée):
Fn_1 <- ecdf(ntabFinal[,1])  # Fonction de répartition empirique
Fn_2 <- ecdf(ntabFinal[,2])  # Fonction de répartition empirique
Fn_3 <- ecdf(ntabFinal[,3])  # Fonction de répartition empirique
plot(Fn_1, verticals = TRUE, do.points = FALSE, main = "Fonction de répartition de la variable RecBro")
plot(Fn_2, verticals = TRUE, do.points = FALSE, main = "Fonction de répartition de la variable Rxt")
plot(Fn_3, verticals = TRUE, do.points = FALSE, main = "Fonction de répartition de la variable TotBusy")
matF <- matrix(NA, nrow = nrow(ntabFinal), ncol = ncol(ntabFinal)-1)     # Création de la matrice des répartition
matF[,1] <- nrow(ntabFinal) / (nrow(ntabFinal)+1) * Fn_1(ntabFinal[,1])  # première colonne (Rescaled distribution)
matF[,2] <- nrow(ntabFinal) / (nrow(ntabFinal)+1) * Fn_2(ntabFinal[,2])  # deuxième colonne (Rescaled distribution)
matF[,3] <- nrow(ntabFinal) / (nrow(ntabFinal)+1) * Fn_3(ntabFinal[,3])  # troisième colonne (Rescaled distribution)
matF
MatntabFinal <- as.matrix(ntabFinal)
# Étape 3: Estimation du paramètre de la copule:
# Copules: 1 = Gaussienne, 2 = Student, 3 = Gumbel, 4 = Clayton, 5 = Frank, 6 = Joe,
coefficient <- function(copule, matrice){
if(copule == 1){
cop <- normalCopula(dim = 3)
t <- "La copule gaussienne possède un paramètre égal à"
} else if(copule == 2){
cop <- tCopula(dim = 3)
t <- "La copule Student possède un paramètre égal à"
} else if(copule == 3){
cop <- gumbelCopula(dim = 3)
t <- "La copule Gumbel possède un paramètre égal à"
} else if(copule == 4){
cop <- claytonCopula(dim = 3)
t <- "La copule Clayton possède un paramètre égal à"
} else if(copule == 5){
cop <- frankCopula(dim = 3)
t <- "La copule de Frank possède un paramètre égal à"
} else if(copule == 6){
cop <- joeCopula(dim = 3)
t <- "La copule de Joe possède un paramètre égal à"
}
fit <- fitCopula(cop, matrice, method = "mpl")
return(list((paste(t, coef(fit), sep = " ")),coef(fit)))
}
coefficient(1,matF)
param_gauss <- unname(coefficient(1,matF)[2][[1]])  # paramètre de la copule gaussienne
coefficient(2,matF)                                 # possède le paramètre ainsi que le degré de liberté!
param_stud <- unname(coefficient(2,matF)[2][[1]])   # paramètres de la copule de Student
coefficient(3,matF)
param_gumb <- unname(coefficient(3,matF)[2][[1]])  # paramètre de la copule Gumbel
coefficient(4,matF)
param_clay <- unname(coefficient(4,matF)[2][[1]])  # paramètre de la copule Clayton
coefficient(5,matF)
param_frank <- unname(coefficient(5,matF)[2][[1]])  # paramètre de la copule Frank
coefficient(6,matF)
param_joe <- unname(coefficient(6,matF)[2][[1]])  # paramètre de la copule Joe
# Calcul de la vraisemblance que l'on cherche à minimiser par la suite à l'aide des critères AIC/BIC
t_max <- nrow(ntabFinal)
Func_gauss = function(x,t) log(dCopula(c(matF[t,1],matF[t,2],matF[t,3]),normalCopula(param_gauss, dim = 3)))
vrais_gauss = sum(sapply(1:t_max, function(t) Func_gauss(x,t)))
Func_stud = function(x,t) log(dCopula(c(matF[t,1],matF[t,2],matF[t,3]),tCopula(param = param_stud[1], df = param_stud[2] , dim = 3)))
vrais_stud = sum(sapply(1:t_max, function(t) Func_stud(x,t)))
Func_gumb = function(x,t) log(dCopula(c(matF[t,1],matF[t,2],matF[t,3]),gumbelCopula(param_gumb, dim = 3)))
vrais_gumb = sum(sapply(1:t_max, function(t) Func_gumb(x,t)))
Func_clay = function(x,t) log(dCopula(c(matF[t,1],matF[t,2],matF[t,3]),claytonCopula(param_clay, dim = 3)))
vrais_clay = sum(sapply(1:t_max, function(t) Func_clay(x,t)))
Func_frank = function(x,t) log(dCopula(c(matF[t,1],matF[t,2],matF[t,3]),frankCopula(param_frank, dim = 3)))
vrais_frank = sum(sapply(1:t_max, function(t) Func_frank(x,t)))
Func_joe = function(x,t) log(dCopula(c(matF[t,1],matF[t,2],matF[t,3]),joeCopula(param_joe, dim = 3)))
vrais_joe = sum(sapply(1:t_max, function(t) Func_joe(x,t)))
#Notons les résultats de vraisemblance dans un même vecteur
logvraisemblance_des_cop = c(vrais_gauss,vrais_stud,vrais_gumb,vrais_clay,vrais_frank,vrais_joe)
# Calculons l'AIC et le BIC
AIC_des_modeles = 2 - 2*(logvraisemblance_des_cop)
BIC_des_modeles = log(nrow(ntabFinal)) - 2 * logvraisemblance_des_cop
tableauAIC <- t(as.data.frame(AIC_des_modeles))
colnames(tableauAIC) <- c("Gauss", "Student", "Gumbel", "Clayton", "Frank", "Joe")
tableauAIC
tableauBIC <- t(as.data.frame(BIC_des_modeles))
colnames(tableauBIC) <- c("Gauss", "Student", "Gumbel", "Clayton", "Frank", "Joe")
tableauBIC
if(isTRUE(which.min(tableauAIC) == which.min(tableauBIC))){
copgagn <- which.min(tableauAIC)
} else
"Nous avons un problème au AIC/BIC"
col5  <- c(1- es_p)
matcop <- cbind(col5,matF)
fit <- vinecop(matF, var_types = c("c","c","c"), par_method = "mle", family_set = "parametric")
summary(fit)
plot(fit)
contour(fit)
fit <- vinecop(matcop, var_types = c("c","c","c"), par_method = "mle", family_set = "parametric")
fit <- vinecop(matcop, var_types = c("d","c","c","c"), par_method = "mle", family_set = "parametric")
fit <- vinecop(matcop, var_types = c("c","c","c","c"), par_method = "mle", family_set = "parametric")
summary(fit)
plot(fit)
contour(fit)
# Importer le jeu de données
donnees <- read.table(file = 'C:/Users/bera1923/Desktop/CeCS - Consultations/Clients et Clientes/Zidane Toffa/donnees.txt', sep = "\t", header = TRUE)
# **** Changer file en utilisant le même jeu de données fournit par courriel ****
#Envoyer Attack en dernière colonne (pour utiliser la fonction de StepWise)
Att <- donnees[,1]                   # Copie la première colonne
donnees <- donnees[,-1]              # Enlève la première colonne
donneesFinal <- cbind(donnees,Att)   # Colle la copie à la fin du tableau
colnames(donneesFinal) <- c("RecBro", "Rxt", "Slot", "Snir", "Tim", "TotBusy", "TotLost", "Att")
# Noms des colonnes abrégés
# Créer un ensemble d'entraînement et de test pour comparer StepWise / Lasso
set.seed(12)
echaEntr <- donneesFinal$Att %>%
createDataPartition(p = 0.8, list = FALSE)    # 80% pour entraînement, 20% pour Test
donneesEntr  <- donneesFinal[echaEntr, ]  # données d'entraînement
donneesTest <- donneesFinal[-echaEntr, ]  # données test
# 1- Utilisons les données d'entraînement pour estimer les paramètres du modèle
# 2- Appliquons le modèle avec les paramètres estimés sur les données test
# Pour le modèle lasso
x <- model.matrix(Att~., donneesEntr)[,-1]   # Sur R, on doit séparer les variables x et y pour Lasso
y <- donneesEntr$Att                         # On obtient x et y ainsi
cv.lasso <- cv.glmnet(x, y, alpha = 1, family = "binomial")
# Fit le modèle final sur les données d'entraînement
model.lasso <- glmnet(x, y, alpha = 1, family = "binomial",
lambda = cv.lasso$lambda.min)
# Coefficients de la régression
coef(model.lasso)
# Prédictions
x.test <- model.matrix(Att ~., donneesTest)[,-1]
prob <- model.lasso %>% predict(newx = x.test)
ClassPred <- ifelse(prob > 0.5, 1, 0)
# Précision
ClassObs <- donneesTest$Att
mean(ClassPred == ClassObs)
# Stepwise model
step.model <- glm(Att ~., data = donneesEntr, family = binomial, control = glm.control(maxit = 50)) %>%
stepAIC(trace = TRUE)
# Coefficients de la régression
coef(step.model)
summary(step.model)
# Prédictions
prob <- predict(step.model, donneesTest, type = "response")
ClassPred <- ifelse(prob > 0.5, 1, 0)
# Précision
ClassObs <- donneesTest$Att
mean(ClassPred==donneesEntr$Att)
# Test de corrélation entre les variables explicatives (après test sur tous les cas possibles)
cor(donneesFinal, method = "pearson")
# Correlation forte entre Rxt et Slot
# Correlation forte entre Rxt et Tim
# Correlation forte entre Rxt et TotLost
# Correlation forte entre Slot et Tim
# Correlation forte entre Slot et TotLost
# Correlation forte entre Snir et TotLost
# Correlation forte entre Tim et TotLost
# Rxt - Slot - TotLost - Tim  semblent fortement corrélés entre elles. (linéairement)
# Test en utilisant seulement les variables RecBro, Rxt, Snir et TotBusy a été fait mais mène à la même conclusion.
### Le modèle Lasso propose d'utiliser les variables explicatives suivantes: RecBro, Rxt, et TotBusy ###
# Créeons notre nouveau tableau de données
col1 <- donneesFinal[,1]  # RecBro
col2 <- donneesFinal[,2]  # Rxt
col3 <- donneesFinal[,6]  # TotBusy
col4 <- donneesFinal[,8]  # Att
ntabFinal <- cbind.data.frame(col1,col2,col3,col4)           # coller les 4 colonnes
colnames(ntabFinal) <- c("RecBro", "Rxt", "TotBusy", "Att")  # renommer les colonnes
ntabFinal
### Partie de l'algorithme dans Article_Copule ###
# Étape 1: Estimation du p (par le pourcentage de succès):
es_p <- mean(ntabFinal[,4])   # Effectue la moyenne des Att (Somme des 0 et 1 divisé en n = 240 attaques)
es_p   # la probabilité d'attaque est de 0.2083333
# Étape 2: Estimation des marginales (par la distribution empirique redimensionnée):
Fn_1 <- ecdf(ntabFinal[,1])  # Fonction de répartition empirique
Fn_2 <- ecdf(ntabFinal[,2])  # Fonction de répartition empirique
Fn_3 <- ecdf(ntabFinal[,3])  # Fonction de répartition empirique
plot(Fn_1, verticals = TRUE, do.points = FALSE, main = "Fonction de répartition de la variable RecBro")
plot(Fn_2, verticals = TRUE, do.points = FALSE, main = "Fonction de répartition de la variable Rxt")
plot(Fn_3, verticals = TRUE, do.points = FALSE, main = "Fonction de répartition de la variable TotBusy")
matF <- matrix(NA, nrow = nrow(ntabFinal), ncol = ncol(ntabFinal)-1)     # Création de la matrice des répartition
matF[,1] <- nrow(ntabFinal) / (nrow(ntabFinal)+1) * Fn_1(ntabFinal[,1])  # première colonne (Rescaled distribution)
matF[,2] <- nrow(ntabFinal) / (nrow(ntabFinal)+1) * Fn_2(ntabFinal[,2])  # deuxième colonne (Rescaled distribution)
matF[,3] <- nrow(ntabFinal) / (nrow(ntabFinal)+1) * Fn_3(ntabFinal[,3])  # troisième colonne (Rescaled distribution)
matF
MatntabFinal <- as.matrix(ntabFinal)
exp(1)
log(100)
2.718282^4.60517
log10(100)
log(100, base = 10)
setwd("C:/Users/bera1923/Desktop/Ateliers_ALEA/Atelier_2")
party <- read.table(file="donnees/party.csv", header=TRUE, sep = ";", row.names = 1)
View(party)
party <- read.table(file="donnees/party.csv", header=TRUE, sep = ";", row.names = 2)
View(party)
party <- read.table(file="donnees/party.csv", header=TRUE, sep = ";")
View(party)
